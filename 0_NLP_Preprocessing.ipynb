{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# Computational Linguistics with Python: Chapter 0\n",
        "# Introduction to NLP & Text Data Processing"
      ],
      "metadata": {
        "id": "JmfUf8cesBvl"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## What is NLP?\n",
        "\n",
        "NLP stands for Natural Language Processing. It is considered a sub-field of AI by some and a standalone discipline by others. Its main goals are to help computers imitate human understanding and generation of natural (human) languages.\n",
        "\n",
        "\n",
        "From ChatGPT to Google Translate, many AI products that we use daily are actually based on NLP practices.\n",
        "\n",
        "\n",
        "\n",
        "I've written these tutorials as smooth, high-level introduction for those who do not have prior Linguistics or NLP experience.\n",
        "\n",
        "\n",
        "\n",
        "While we are going to start off with something bit heavier on the programming side, bear in mind these tutorials were designed to help you learn Linguistics as well as NLP. Therefore, expect well-defined theoretical explanations as well as Python examples."
      ],
      "metadata": {
        "id": "8inmST3hsHGy"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Preparing Text Data for NLP\n",
        "\n",
        "As you probably already know, it's not easy for machines to work with text data. In this first chapter, we will take a look at various data preprocessing steps that will make our text data more suitable for NLP tasks.  "
      ],
      "metadata": {
        "id": "KyOXUsLouS8v"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Tokenization\n",
        "\n",
        "Tokenization is the process through which we split the text into smaller pieces called tokens. Let's take a look at an example:"
      ],
      "metadata": {
        "id": "r_SI4JwswMLC"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Install nltk if you don't have it\n",
        "!pip install nltk"
      ],
      "metadata": {
        "id": "ErNwRPe2xEwS"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import nltk"
      ],
      "metadata": {
        "id": "HSzQbSCFxIqD"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Download punkt\n",
        "nltk.download('punkt')"
      ],
      "metadata": {
        "id": "bcYYt6TOYray",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "f140c5a3-e474-47c8-a539-c0f339f0b507"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Unzipping tokenizers/punkt.zip.\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {},
          "execution_count": 2
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "nkkb7FgEqgBh",
        "outputId": "f0cae7aa-afe7-45d8-ff6a-d86bc894ed37"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "['Have', 'I', 'not', 'done', 'enough', 'for', 'you', '?']\n"
          ]
        }
      ],
      "source": [
        "from nltk.tokenize import word_tokenize\n",
        "\n",
        "# Full text\n",
        "text = \"Have I not done enough for you?\"\n",
        "\n",
        "# Break into tokens\n",
        "tokens = word_tokenize(text)\n",
        "\n",
        "print(tokens)"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Normalization\n",
        "\n",
        "Normalization is performed to make your text data consistent and, therefore, more suitable to be processed by algorithms.\n",
        "\n",
        "It often involves lowercasing the words and the removal of punctuation marks."
      ],
      "metadata": {
        "id": "j4N45OCoxwY2"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Lowercase all words\n",
        "\n",
        "my_text = \"OH MAN! This is awesome!\"\n",
        "\n",
        "my_text = my_text.lower()\n",
        "\n",
        "print(my_text)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "PI8RY3HdyUCd",
        "outputId": "8d1b8779-ff57-4f33-b5bc-62999df91178"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "oh man! this is awesome!\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Remove Punctuation\n",
        "\n",
        "import string\n",
        "text_without_punctuation = my_text.translate(str.maketrans('', '', string.punctuation))\n",
        "print(text_without_punctuation)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "EICjFAT8zhjr",
        "outputId": "f72044b3-b49c-46b0-9530-d999d7aeb8fe"
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "oh man this is awesome\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Stopword Removal\n",
        "\n",
        "Stopwords are frequently used words which don't always carry a crucial meaning. While there can be certain exceptions, it is common NLP practice to remove them."
      ],
      "metadata": {
        "id": "WspBrhsQ0Gnb"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Download stopwords\n",
        "nltk.download('stopwords')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "sRc1HnOA1JJa",
        "outputId": "3e41bcb5-9966-4bac-f7d6-dc92df2d9078"
      },
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Unzipping corpora/stopwords.zip.\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {},
          "execution_count": 6
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from nltk.corpus import stopwords\n",
        "\n",
        "stop_words = set(stopwords.words('english'))\n",
        "text = \"There is a red car in the gallery\"\n",
        "tokens = word_tokenize(text)\n",
        "filtered_tokens = [word for word in tokens if not word.lower() in stop_words]\n",
        "print(filtered_tokens)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "InaNTW9G1EFa",
        "outputId": "4b363ff4-5d54-4d70-f3c9-6a118ddb0705"
      },
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "['red', 'car', 'gallery']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Stemming\n",
        "\n",
        "Stemming basically means reaching the root (stem) of a sentence.\n",
        "\n",
        "The main objective of stemming is to reduce the negative effects of abundance, by ensuring that similar topics are treated the same way.\n",
        "\n"
      ],
      "metadata": {
        "id": "SnWxmolH1Xxg"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from nltk.stem import PorterStemmer\n",
        "\n",
        "stemmer = PorterStemmer()\n",
        "words = [\"study\",\"studies\"]\n",
        "stems = [stemmer.stem(word) for word in words]\n",
        "print(stems)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "NdViwh1XWh2M",
        "outputId": "781b9670-64bb-4434-fe5d-afd43023ddd2"
      },
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "['studi', 'studi']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "It seems to be working nice, let's try something else:"
      ],
      "metadata": {
        "id": "wtLdu08rZTuS"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "new_words = [\"studies\",\"geese\"]\n",
        "new_stems = [stemmer.stem(word) for word in new_words]\n",
        "print(new_stems)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Iy3VnQRVZYXU",
        "outputId": "f3abc2d3-649c-4d7a-9d4e-2ccc054f14ce"
      },
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "['studi', 'gees']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "There is a problem. It fails to be effective in certain cases, like plurals.\n",
        "\n",
        "How can we improve this process?\n",
        "\n",
        "With lemmatization."
      ],
      "metadata": {
        "id": "QipfdgNCWn5r"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Lemmatization\n",
        "\n",
        "Lemmatization has the same goal as stemming, however it achieves that goal in a more efficient manner.\n",
        "\n"
      ],
      "metadata": {
        "id": "gZpkgYQCW_Ot"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "nltk.download('wordnet')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "o0tdugVvYNNp",
        "outputId": "91cabb7f-1381-4ff1-affd-1116dadbaa67"
      },
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package wordnet to /root/nltk_data...\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {},
          "execution_count": 10
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from nltk.stem import WordNetLemmatizer\n",
        "\n",
        "lemmatizer = WordNetLemmatizer()\n",
        "comp_words = [\"studies\",\"geese\"]\n",
        "lemmas = [lemmatizer.lemmatize(word) for word in comp_words]\n",
        "print(lemmas)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "OQSn_3wCXfOV",
        "outputId": "b2348ca5-2613-42c5-8793-e039240c7f37"
      },
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "['study', 'goose']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Conclusion\n",
        "\n",
        "Preprocessing text data is not the most entertaining thing you can do in the world.\n",
        "\n",
        "However, if you do care about the quality of your text-based analysis or want to overall understand the fundamentals of computational linguistics, you should pay attention to it.\n",
        "\n",
        "In the next chapter, we will analyze the structure of sentences by delving into the delightful topic of Syntactic Analysis.\n",
        "\n"
      ],
      "metadata": {
        "id": "21CKV6REiWZB"
      }
    }
  ]
}